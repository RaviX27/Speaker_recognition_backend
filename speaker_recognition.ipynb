{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0GGVzcujexYb"
      },
      "outputs": [],
      "source": [
        "# Import the required packages\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import shutil\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Audio\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzofXW9tkByq",
        "outputId": "b5606908-d444-469e-caf8-f00d0a414a42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data directories\n",
        "data_directory = \"/content/drive/MyDrive/16000_pcm_speeches\"\n",
        "audio_folder = \"audio\"\n",
        "noise_folder = \"noise\"\n",
        "\n",
        "audio_path = os.path.join(data_directory, audio_folder)\n",
        "noise_path = os.path.join(data_directory, noise_folder)"
      ],
      "metadata": {
        "id": "ZdDEe9VFgPC0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HS_bFOvanX2j",
        "outputId": "28c45412-c0e7-4b60-8ccd-1a225e59b7ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/16000_pcm_speeches/audio'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_split = 0.1\n",
        "\n",
        "shuffle_seed = 43\n",
        "\n",
        "sample_rate = 16000\n",
        "\n",
        "scale = 0.5\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "hT1zNa8YqLH0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "arrange audio and noise"
      ],
      "metadata": {
        "id": "jMUfs4jwrcQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in os.listdir(data_directory):\n",
        "    if os.path.isdir(os.path.join(data_directory, folder)):\n",
        "        if folder in [audio_folder, noise_folder]:\n",
        "\n",
        "            continue\n",
        "        elif folder in [\"other\", \"_background_noise_\"]:\n",
        "\n",
        "            shutil.move(\n",
        "                os.path.join(data_directory, folder),\n",
        "                os.path.join(noise_path, folder),\n",
        "            )\n",
        "        else:\n",
        "            shutil.move(\n",
        "                os.path.join(data_directory, folder),\n",
        "                os.path.join(audio_path, folder),\n",
        "            )"
      ],
      "metadata": {
        "id": "IkULQwTtqO6E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the list of all noise files"
      ],
      "metadata": {
        "id": "SfN73f-Yri7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_paths = []\n",
        "for subdir in os.listdir(noise_path):\n",
        "    subdir_path = Path(noise_path) / subdir\n",
        "    if os.path.isdir(subdir_path):\n",
        "        noise_paths += [\n",
        "            os.path.join(subdir_path, filepath)\n",
        "            for filepath in os.listdir(subdir_path)\n",
        "            if filepath.endswith(\".wav\")\n",
        "        ]"
      ],
      "metadata": {
        "id": "KAb_x7xSrfec"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_FAywnnrn3n",
        "outputId": "d7a58887-ef09-4c53-ca80-a32ec4efb9e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/16000_pcm_speeches/noise/other/pink_noise.wav',\n",
              " '/content/drive/MyDrive/16000_pcm_speeches/noise/other/exercise_bike.wav',\n",
              " '/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/doing_the_dishes.wav',\n",
              " '/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/10convert.com_Audience-Claps_daSG5fwdA7o.wav',\n",
              " '/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/dude_miaowing.wav',\n",
              " '/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/running_tap.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split noise into chunks of 16,000 steps each"
      ],
      "metadata": {
        "id": "nNxcXOS1rwL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "command = (\n",
        "    \"for dir in `ls -1 \" + noise_path + \"`; do \"\n",
        "    \"for file in `ls -1 \" + noise_path + \"/$dir/*.wav`; do \"\n",
        "    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n",
        "    \"$file | grep sample_rate | cut -f2 -d=`; \"\n",
        "    \"if [ $sample_rate -ne 16000 ]; then \"\n",
        "    \"ffmpeg -hide_banner -loglevel panic -y \"\n",
        "    \"-i $file -ar 16000 temp.wav; \"\n",
        "    \"mv temp.wav $file; \"\n",
        "    \"fi; done; done\"\n",
        ")"
      ],
      "metadata": {
        "id": "GEz2kWTurw1_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(command)\n",
        "def load_noise_sample(path):\n",
        "    sample, sampling_rate = tf.audio.decode_wav(\n",
        "        tf.io.read_file(path), desired_channels=1\n",
        "    )\n",
        "    if sampling_rate == sample_rate:\n",
        "        slices = int(sample.shape[0] / sample_rate)\n",
        "        sample = tf.split(sample[: slices * sample_rate], slices)\n",
        "        return sample\n",
        "    else:\n",
        "        print(\"Sampling rate for\",path, \"is incorrect\")\n",
        "        return None\n",
        "\n",
        "\n",
        "noises = []\n",
        "for path in noise_paths:\n",
        "    sample = load_noise_sample(path)\n",
        "    if sample:\n",
        "        noises.extend(sample)\n",
        "noises = tf.stack(noises)"
      ],
      "metadata": {
        "id": "g6NXPke2r2Qf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET GENERATION"
      ],
      "metadata": {
        "id": "iGtRcP1qr5pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def paths_and_labels_to_dataset(audio_paths, labels):\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    return tf.data.Dataset.zip((audio_ds, label_ds))"
      ],
      "metadata": {
        "id": "-3hUX5RNr3B9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def path_to_audio(path):\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1, sample_rate)\n",
        "    return audio"
      ],
      "metadata": {
        "id": "WxhTn1Rmr8s0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add noise to dataset"
      ],
      "metadata": {
        "id": "fkRb2qKosAuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(audio, noises=None, scale=0.5):\n",
        "    if noises is not None:\n",
        "        tf_rnd = tf.random.uniform(\n",
        "            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
        "        )\n",
        "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
        "\n",
        "        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
        "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
        "\n",
        "        audio = audio + noise * prop * scale\n",
        "\n",
        "    return audio"
      ],
      "metadata": {
        "id": "YCdMN5Blr-1-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_fft(audio):\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    fft = tf.signal.fft(\n",
        "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
        "    )\n",
        "    fft = tf.expand_dims(fft, axis=-1)\n",
        "\n",
        "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])"
      ],
      "metadata": {
        "id": "-MToPHNCsF3E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = os.listdir(audio_path)\n",
        "print(class_names,)\n",
        "\n",
        "audio_paths = []\n",
        "labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Speaker:\",(name))\n",
        "    dir_path = Path(audio_path) / name\n",
        "    speaker_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    audio_paths += speaker_sample_paths\n",
        "    labels += [label] * len(speaker_sample_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdl-UF3qsIFm",
        "outputId": "93b43a8d-78b0-472f-fb7f-481181e2aa97"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nelson_Mandela', 'Jens_Stoltenberg', 'Julia_Gillard', 'Benjamin_Netanyau', 'Magaret_Tarcher']\n",
            "Speaker: Nelson_Mandela\n",
            "Speaker: Jens_Stoltenberg\n",
            "Speaker: Julia_Gillard\n",
            "Speaker: Benjamin_Netanyau\n",
            "Speaker: Magaret_Tarcher\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle to generate random data\n",
        "rng = np.random.RandomState(shuffle_seed)\n",
        "rng.shuffle(audio_paths)\n",
        "rng = np.random.RandomState(shuffle_seed)\n",
        "rng.shuffle(labels)"
      ],
      "metadata": {
        "id": "b-ifFK2zsRa_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation\n",
        "num_val_samples = int(valid_split * len(audio_paths))\n",
        "train_audio_paths = audio_paths[:-num_val_samples]\n",
        "train_labels = labels[:-num_val_samples]\n",
        "\n",
        "\n",
        "valid_audio_paths = audio_paths[-num_val_samples:]\n",
        "valid_labels = labels[-num_val_samples:]"
      ],
      "metadata": {
        "id": "OJA2_l-lsR-t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets, one for training and the other for validation\n",
        "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
        "train_ds = train_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
        "    batch_size\n",
        ")\n",
        "\n",
        "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=shuffle_seed).batch(32)"
      ],
      "metadata": {
        "id": "77TmmMAssT1_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature Extraction"
      ],
      "metadata": {
        "id": "QhB1gRXesXtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add noise to the training set\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (add_noise(x, noises, scale=scale), y),\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        ")\n",
        "\n",
        "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "\n",
        "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_ds = valid_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "abowvYWmsWKP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "uvigf8wj2x8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D"
      ],
      "metadata": {
        "id": "DNcNlmXlscnE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters, conv_num = 3, activation = \"relu\"):\n",
        "    s = keras.layers.Conv1D(filters, 1, padding = \"same\")(x)\n",
        "\n",
        "    for i in range(conv_num - 1):\n",
        "        x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
        "        x = keras.layers.Activation(activation)(x)\n",
        "\n",
        "    x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
        "    x = keras.layers.Add()([x, s])\n",
        "    x = keras.layers.Activation(activation)(x)\n",
        "\n",
        "    return keras.layers.MaxPool1D(pool_size = 2, strides = 2)(x)\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = keras.layers.Input(shape = input_shape, name = \"input\")\n",
        "\n",
        "    x = residual_block(inputs, 16, 2)\n",
        "    x = residual_block(inputs, 32, 2)\n",
        "    x = residual_block(inputs, 64, 3)\n",
        "    x = residual_block(inputs, 128, 3)\n",
        "    x = residual_block(inputs, 128, 3)\n",
        "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(num_classes, activation = \"softmax\", name = \"output\")(x)\n",
        "\n",
        "    return keras.models.Model(inputs = inputs, outputs = outputs)"
      ],
      "metadata": {
        "id": "LIHA_MV-2xkI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model((sample_rate // 2, 1), len(class_names))"
      ],
      "metadata": {
        "id": "XAwQKmqq25ii"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_save_filename = \"model.h5\"\n",
        "\n",
        "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(model_save_filename, monitor=\"val_accuracy\", save_best_only=True)"
      ],
      "metadata": {
        "id": "g6hMt5YM27rq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=valid_ds\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyF_mjOG2_TU",
        "outputId": "ea162e69-e938-4ea3-b80c-8c8822c83b2e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "53/53 [==============================] - 869s 16s/step - loss: 14.8192 - accuracy: 0.5980 - val_loss: 0.3424 - val_accuracy: 0.8547\n",
            "Epoch 2/50\n",
            "53/53 [==============================] - 42s 732ms/step - loss: 0.1920 - accuracy: 0.9259 - val_loss: 0.1679 - val_accuracy: 0.9467\n",
            "Epoch 3/50\n",
            "53/53 [==============================] - 39s 674ms/step - loss: 0.1340 - accuracy: 0.9492 - val_loss: 0.1045 - val_accuracy: 0.9733\n",
            "Epoch 4/50\n",
            "53/53 [==============================] - 39s 704ms/step - loss: 0.1304 - accuracy: 0.9545 - val_loss: 0.0872 - val_accuracy: 0.9707\n",
            "Epoch 5/50\n",
            "53/53 [==============================] - 37s 666ms/step - loss: 0.0822 - accuracy: 0.9720 - val_loss: 0.0733 - val_accuracy: 0.9760\n",
            "Epoch 6/50\n",
            "53/53 [==============================] - 37s 662ms/step - loss: 0.0790 - accuracy: 0.9711 - val_loss: 0.0659 - val_accuracy: 0.9813\n",
            "Epoch 7/50\n",
            "53/53 [==============================] - 41s 713ms/step - loss: 0.0755 - accuracy: 0.9744 - val_loss: 0.0569 - val_accuracy: 0.9827\n",
            "Epoch 8/50\n",
            "53/53 [==============================] - 39s 709ms/step - loss: 0.0639 - accuracy: 0.9772 - val_loss: 0.0812 - val_accuracy: 0.9760\n",
            "Epoch 9/50\n",
            "53/53 [==============================] - 39s 703ms/step - loss: 0.0687 - accuracy: 0.9772 - val_loss: 0.0736 - val_accuracy: 0.9813\n",
            "Epoch 10/50\n",
            "53/53 [==============================] - 40s 718ms/step - loss: 0.0777 - accuracy: 0.9759 - val_loss: 0.0596 - val_accuracy: 0.9867\n",
            "Epoch 11/50\n",
            "53/53 [==============================] - 37s 671ms/step - loss: 0.0453 - accuracy: 0.9831 - val_loss: 0.0822 - val_accuracy: 0.9813\n",
            "Epoch 12/50\n",
            "53/53 [==============================] - 43s 772ms/step - loss: 0.0539 - accuracy: 0.9813 - val_loss: 0.0993 - val_accuracy: 0.9747\n",
            "Epoch 13/50\n",
            "53/53 [==============================] - 41s 733ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0823 - val_accuracy: 0.9827\n",
            "Epoch 14/50\n",
            "53/53 [==============================] - 39s 704ms/step - loss: 0.0754 - accuracy: 0.9776 - val_loss: 0.1030 - val_accuracy: 0.9720\n",
            "Epoch 15/50\n",
            "53/53 [==============================] - 37s 669ms/step - loss: 0.0629 - accuracy: 0.9785 - val_loss: 0.0691 - val_accuracy: 0.9800\n",
            "Epoch 16/50\n",
            "53/53 [==============================] - 39s 679ms/step - loss: 0.0375 - accuracy: 0.9859 - val_loss: 0.0529 - val_accuracy: 0.9853\n",
            "Epoch 17/50\n",
            "53/53 [==============================] - 42s 760ms/step - loss: 0.0332 - accuracy: 0.9874 - val_loss: 0.0803 - val_accuracy: 0.9867\n",
            "Epoch 18/50\n",
            "53/53 [==============================] - 39s 707ms/step - loss: 0.0239 - accuracy: 0.9919 - val_loss: 0.0559 - val_accuracy: 0.9880\n",
            "Epoch 19/50\n",
            "53/53 [==============================] - 37s 671ms/step - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.0668 - val_accuracy: 0.9853\n",
            "Epoch 20/50\n",
            "53/53 [==============================] - 41s 705ms/step - loss: 0.0317 - accuracy: 0.9893 - val_loss: 0.0668 - val_accuracy: 0.9840\n",
            "Epoch 21/50\n",
            "53/53 [==============================] - 41s 737ms/step - loss: 0.0292 - accuracy: 0.9904 - val_loss: 0.0703 - val_accuracy: 0.9840\n",
            "Epoch 22/50\n",
            "53/53 [==============================] - 47s 849ms/step - loss: 0.0512 - accuracy: 0.9831 - val_loss: 0.1285 - val_accuracy: 0.9760\n",
            "Epoch 23/50\n",
            "53/53 [==============================] - 40s 724ms/step - loss: 0.0333 - accuracy: 0.9898 - val_loss: 0.0357 - val_accuracy: 0.9893\n",
            "Epoch 24/50\n",
            "53/53 [==============================] - 41s 746ms/step - loss: 0.0216 - accuracy: 0.9914 - val_loss: 0.0955 - val_accuracy: 0.9840\n",
            "Epoch 25/50\n",
            "53/53 [==============================] - 43s 775ms/step - loss: 0.0342 - accuracy: 0.9896 - val_loss: 0.0407 - val_accuracy: 0.9907\n",
            "Epoch 26/50\n",
            "53/53 [==============================] - 44s 799ms/step - loss: 0.0158 - accuracy: 0.9941 - val_loss: 0.1002 - val_accuracy: 0.9787\n",
            "Epoch 27/50\n",
            "53/53 [==============================] - 40s 721ms/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.1459 - val_accuracy: 0.9747\n",
            "Epoch 28/50\n",
            "53/53 [==============================] - 40s 731ms/step - loss: 0.0310 - accuracy: 0.9902 - val_loss: 0.0788 - val_accuracy: 0.9893\n",
            "Epoch 29/50\n",
            "53/53 [==============================] - 38s 680ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0470 - val_accuracy: 0.9867\n",
            "Epoch 30/50\n",
            "53/53 [==============================] - 41s 735ms/step - loss: 0.0159 - accuracy: 0.9941 - val_loss: 0.1164 - val_accuracy: 0.9800\n",
            "Epoch 31/50\n",
            "53/53 [==============================] - 41s 718ms/step - loss: 0.0118 - accuracy: 0.9953 - val_loss: 0.0425 - val_accuracy: 0.9933\n",
            "Epoch 32/50\n",
            "53/53 [==============================] - 39s 689ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.0692 - val_accuracy: 0.9920\n",
            "Epoch 33/50\n",
            "53/53 [==============================] - 39s 689ms/step - loss: 0.0329 - accuracy: 0.9901 - val_loss: 0.1443 - val_accuracy: 0.9680\n",
            "Epoch 34/50\n",
            "53/53 [==============================] - 41s 747ms/step - loss: 0.0791 - accuracy: 0.9794 - val_loss: 0.0921 - val_accuracy: 0.9813\n",
            "Epoch 35/50\n",
            "53/53 [==============================] - 40s 726ms/step - loss: 0.0264 - accuracy: 0.9905 - val_loss: 0.0748 - val_accuracy: 0.9853\n",
            "Epoch 36/50\n",
            "53/53 [==============================] - 38s 688ms/step - loss: 0.0145 - accuracy: 0.9944 - val_loss: 0.0851 - val_accuracy: 0.9893\n",
            "Epoch 37/50\n",
            "53/53 [==============================] - 41s 718ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.1331 - val_accuracy: 0.9800\n",
            "Epoch 38/50\n",
            "53/53 [==============================] - 42s 756ms/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 0.0809 - val_accuracy: 0.9853\n",
            "Epoch 39/50\n",
            "53/53 [==============================] - 38s 683ms/step - loss: 0.0192 - accuracy: 0.9930 - val_loss: 0.0865 - val_accuracy: 0.9827\n",
            "Epoch 40/50\n",
            "53/53 [==============================] - 39s 702ms/step - loss: 0.0117 - accuracy: 0.9953 - val_loss: 0.0632 - val_accuracy: 0.9853\n",
            "Epoch 41/50\n",
            "53/53 [==============================] - 39s 707ms/step - loss: 0.0196 - accuracy: 0.9944 - val_loss: 0.1411 - val_accuracy: 0.9800\n",
            "Epoch 42/50\n",
            "53/53 [==============================] - 38s 678ms/step - loss: 0.0382 - accuracy: 0.9890 - val_loss: 0.0634 - val_accuracy: 0.9853\n",
            "Epoch 43/50\n",
            "53/53 [==============================] - 43s 760ms/step - loss: 0.0241 - accuracy: 0.9919 - val_loss: 0.0378 - val_accuracy: 0.9907\n",
            "Epoch 44/50\n",
            "53/53 [==============================] - 40s 710ms/step - loss: 0.0270 - accuracy: 0.9917 - val_loss: 0.0630 - val_accuracy: 0.9853\n",
            "Epoch 45/50\n",
            "53/53 [==============================] - 41s 720ms/step - loss: 0.0147 - accuracy: 0.9947 - val_loss: 0.1164 - val_accuracy: 0.9760\n",
            "Epoch 46/50\n",
            "53/53 [==============================] - 40s 683ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.0687 - val_accuracy: 0.9867\n",
            "Epoch 47/50\n",
            "53/53 [==============================] - 39s 697ms/step - loss: 0.0151 - accuracy: 0.9944 - val_loss: 0.0813 - val_accuracy: 0.9880\n",
            "Epoch 48/50\n",
            "53/53 [==============================] - 39s 709ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.1198 - val_accuracy: 0.9827\n",
            "Epoch 49/50\n",
            "53/53 [==============================] - 37s 669ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 0.0944 - val_accuracy: 0.9840\n",
            "Epoch 50/50\n",
            "53/53 [==============================] - 39s 693ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.0505 - val_accuracy: 0.9907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of model:\",model.evaluate(valid_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOtbKnmG3fvW",
        "outputId": "664709ed-9c72-4d51-faef-48d77cca0f23"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 5s 195ms/step - loss: 0.0505 - accuracy: 0.9907\n",
            "Accuracy of model: [0.0504831001162529, 0.9906666874885559]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_filename = \"model.h5\""
      ],
      "metadata": {
        "id": "dcbOELfHGGCi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict"
      ],
      "metadata": {
        "id": "u8Xr3kU0_IRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLES_TO_DISPLAY = 10\n",
        "\n",
        "test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "test_ds = test_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
        "    batch_size\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=scale), y))\n",
        "\n",
        "for audios, labels in test_ds.take(1):\n",
        "    ffts = audio_to_fft(audios)\n",
        "    y_pred = model.predict(ffts)\n",
        "    rnd = np.random.randint(0, batch_size, SAMPLES_TO_DISPLAY)\n",
        "    audios = audios.numpy()[rnd, :, :]\n",
        "    labels = labels.numpy()[rnd]\n",
        "    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "    for index in range(SAMPLES_TO_DISPLAY):\n",
        "        print(\n",
        "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[labels[index]],\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[y_pred[index]],\n",
        "            )\n",
        "        )\n",
        "        if labels[index] ==y_pred[index]:\n",
        "            print(\"Welcome\")\n",
        "        else:\n",
        "            print(\"Sorry\")\n",
        "        print(\"The speaker is\" if labels[index] == y_pred[index] else \"\", class_names[y_pred[index]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zX3B3at3N95",
        "outputId": "5fffeaf0-4050-4745-aee9-7e514bc41b42"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 46ms/step\n",
            "Speaker:\u001b[92m Nelson_Mandela\u001b[0m\tPredicted:\u001b[92m Nelson_Mandela\u001b[0m\n",
            "Welcome\n",
            "The speaker is Nelson_Mandela\n",
            "Speaker:\u001b[92m Magaret_Tarcher\u001b[0m\tPredicted:\u001b[92m Magaret_Tarcher\u001b[0m\n",
            "Welcome\n",
            "The speaker is Magaret_Tarcher\n",
            "Speaker:\u001b[92m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[92m Jens_Stoltenberg\u001b[0m\n",
            "Welcome\n",
            "The speaker is Jens_Stoltenberg\n",
            "Speaker:\u001b[92m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[92m Jens_Stoltenberg\u001b[0m\n",
            "Welcome\n",
            "The speaker is Jens_Stoltenberg\n",
            "Speaker:\u001b[92m Benjamin_Netanyau\u001b[0m\tPredicted:\u001b[92m Benjamin_Netanyau\u001b[0m\n",
            "Welcome\n",
            "The speaker is Benjamin_Netanyau\n",
            "Speaker:\u001b[92m Magaret_Tarcher\u001b[0m\tPredicted:\u001b[92m Magaret_Tarcher\u001b[0m\n",
            "Welcome\n",
            "The speaker is Magaret_Tarcher\n",
            "Speaker:\u001b[92m Nelson_Mandela\u001b[0m\tPredicted:\u001b[92m Nelson_Mandela\u001b[0m\n",
            "Welcome\n",
            "The speaker is Nelson_Mandela\n",
            "Speaker:\u001b[92m Julia_Gillard\u001b[0m\tPredicted:\u001b[92m Julia_Gillard\u001b[0m\n",
            "Welcome\n",
            "The speaker is Julia_Gillard\n",
            "Speaker:\u001b[92m Magaret_Tarcher\u001b[0m\tPredicted:\u001b[92m Magaret_Tarcher\u001b[0m\n",
            "Welcome\n",
            "The speaker is Magaret_Tarcher\n",
            "Speaker:\u001b[92m Benjamin_Netanyau\u001b[0m\tPredicted:\u001b[92m Benjamin_Netanyau\u001b[0m\n",
            "Welcome\n",
            "The speaker is Benjamin_Netanyau\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predcit the speaker from the test dataset for real time pred."
      ],
      "metadata": {
        "id": "52aRQYjb_X-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def paths_to_dataset(audio_paths):\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    return tf.data.Dataset.zip((path_ds))\n",
        "\n",
        "def predict(path, labels):\n",
        "    test = paths_and_labels_to_dataset(path, labels)\n",
        "\n",
        "\n",
        "    test = test.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
        "    batch_size\n",
        "    )\n",
        "    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "    test = test.map(lambda x, y: (add_noise(x, noises, scale=scale), y))\n",
        "\n",
        "    for audios, labels in test.take(1):\n",
        "        ffts = audio_to_fft(audios)\n",
        "        y_pred = model.predict(ffts)\n",
        "        rnd = np.random.randint(0, 1, 1)\n",
        "        audios = audios.numpy()[rnd, :]\n",
        "        labels = labels.numpy()[rnd]\n",
        "        y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "    for index in range(1):\n",
        "            print(\n",
        "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "            \"[92m\",y_pred[index],\n",
        "                \"[92m\", y_pred[index]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            print(\"Speaker Predicted:\",class_names[y_pred[index]])"
      ],
      "metadata": {
        "id": "KBCY8dum_TmT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = [\"/content/drive/MyDrive/16000_pcm_speeches/audio/Julia_Gillard/1000.wav\"]\n",
        "labels = [\"unknown\"]\n",
        "try:\n",
        "    predict(path, labels)\n",
        "except:\n",
        "    print(\"Error! Check if the file correctly passed or not!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajyVZAq3_eI0",
        "outputId": "85bff165-b1b7-45fe-c929-042d07db2272"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 204ms/step\n",
            "Speaker:\u001b[92m 2\u001b[0m\tPredicted:\u001b[92m 2\u001b[0m\n",
            "Speaker Predicted: Julia_Gillard\n"
          ]
        }
      ]
    }
  ]
}